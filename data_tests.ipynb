{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14ef0ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from AudioDataset import AudioDataset\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "780fb661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device for GPU acceleration.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device for GPU acceleration.\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device for GPU acceleration.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU device found. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de637114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.csv  – class % share\n",
      "label\n",
      "unknown    63.32\n",
      "stop        3.67\n",
      "on          3.63\n",
      "go          3.62\n",
      "yes         3.62\n",
      "no          3.60\n",
      "right       3.60\n",
      "up          3.59\n",
      "down        3.58\n",
      "left        3.58\n",
      "off         3.58\n",
      "silence     0.61\n",
      "Name: proportion, dtype: float64\n",
      "test.csv  – class % share\n",
      "label\n",
      "unknown    63.321920\n",
      "stop        3.667030\n",
      "on          3.626177\n",
      "go          3.620341\n",
      "yes         3.618395\n",
      "no          3.604778\n",
      "right       3.602832\n",
      "up          3.585324\n",
      "down        3.583379\n",
      "left        3.577543\n",
      "off         3.577543\n",
      "silence     0.614738\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ── % share of every class in the CSV ────────────────────────────────\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./data/train.csv\")      # adjust path if needed\n",
    "pct = df[\"label\"].value_counts(normalize=True)*100\n",
    "print(\"train.csv  – class % share\"); print(pct.round(2))\n",
    "print(\"test.csv  – class % share\"); print(df[\"label\"].value_counts(normalize=True)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c279cd79",
   "metadata": {},
   "source": [
    "## without balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1153e737",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = AudioDataset(\"./data/train.csv\", \"./data_raw/train/audio\")\n",
    "train_loader = DataLoader(train_set, batch_size=256, shuffle=True, drop_last=True)\n",
    "\n",
    "test_set = AudioDataset(\"./data/test.csv\", \"./data_raw/train/audio\")\n",
    "test_loader = DataLoader(test_set, batch_size=256, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8277a4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class  0:  3.62% (1852/51200)\n",
      "class  1:  3.61% (1846/51200)\n",
      "class  2:  3.58% (1833/51200)\n",
      "class  3:  3.58% (1835/51200)\n",
      "class  4:  3.58% (1833/51200)\n",
      "class  5:  3.61% (1846/51200)\n",
      "class  6:  3.62% (1854/51200)\n",
      "class  7:  3.58% (1831/51200)\n",
      "class  8:  3.67% (1878/51200)\n",
      "class  9:  3.61% (1849/51200)\n",
      "class 10:  0.62% (315/51200)\n",
      "class 11: 63.34% (32428/51200)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "\n",
    "# ─── count labels in one epoch ─────────────────────────────────────────\n",
    "label_counts = Counter()\n",
    "total_samples = 0\n",
    "\n",
    "for specs, labels in train_loader:\n",
    "    labels = labels.cpu().tolist()\n",
    "    label_counts.update(labels)\n",
    "    total_samples += len(labels)\n",
    "\n",
    "# ─── print percentage per class ─────────────────────────────────────────\n",
    "for cls_idx, count in sorted(label_counts.items()):\n",
    "    pct = count / total_samples * 100\n",
    "    print(f\"class {cls_idx:2d}: {pct:5.2f}% ({count}/{total_samples})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3a430e",
   "metadata": {},
   "source": [
    "## with equal balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80953d45",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 27\u001b[0m\n\u001b[1;32m     19\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     20\u001b[0m     train_set,\n\u001b[1;32m     21\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m     22\u001b[0m     sampler\u001b[38;5;241m=\u001b[39msampler,\n\u001b[1;32m     23\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# ─── class‑weighted loss to keep natural skew partially ─────────────────\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m counts\u001b[38;5;241m.\u001b[39mfloat())\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevice\u001b[49m)\n\u001b[1;32m     28\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mminimum(class_weights, class_weights\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m*\u001b[39m max_mult)\n\u001b[1;32m     29\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(weight\u001b[38;5;241m=\u001b[39mclass_weights)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "# ─── prepare dataset ──────────────────────────────────────────────────────\n",
    "train_set = AudioDataset(\"./data/train.csv\", \"./data_raw/train/audio\")\n",
    "\n",
    "# ─── compute sample weights for balanced sampling ────────────────────────\n",
    "labels = [lbl for _, lbl in train_set]             # list of ints\n",
    "class_counts = torch.bincount(torch.tensor(labels))\n",
    "class_weights = 1.0 / class_counts.float()          # weight per class\n",
    "sample_weights = [class_weights[lbl] for lbl in labels]\n",
    "\n",
    "# ─── sampler & loader ─────────────────────────────────────────────────────\n",
    "sampler = WeightedRandomSampler(\n",
    "    sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=256,\n",
    "    sampler=sampler,     # use sampler instead of shuffle\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# ─── test loader unchanged ────────────────────────────────────────────────\n",
    "test_set = AudioDataset(\"./data/test.csv\", \"./data_raw/train/audio\")\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36837a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class  0:  8.37% (4284/51200)\n",
      "class  1:  8.31% (4257/51200)\n",
      "class  2:  8.17% (4181/51200)\n",
      "class  3:  8.28% (4239/51200)\n",
      "class  4:  8.39% (4296/51200)\n",
      "class  5:  8.26% (4230/51200)\n",
      "class  6:  8.30% (4248/51200)\n",
      "class  7:  8.29% (4247/51200)\n",
      "class  8:  8.25% (4223/51200)\n",
      "class  9:  8.34% (4270/51200)\n",
      "class 10:  8.68% (4445/51200)\n",
      "class 11:  8.36% (4280/51200)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "\n",
    "# ─── count labels in one epoch ─────────────────────────────────────────\n",
    "label_counts = Counter()\n",
    "total_samples = 0\n",
    "\n",
    "for specs, labels in train_loader:\n",
    "    labels = labels.cpu().tolist()\n",
    "    label_counts.update(labels)\n",
    "    total_samples += len(labels)\n",
    "\n",
    "# ─── print percentage per class ─────────────────────────────────────────\n",
    "for cls_idx, count in sorted(label_counts.items()):\n",
    "    pct = count / total_samples * 100\n",
    "    print(f\"class {cls_idx:2d}: {pct:5.2f}% ({count}/{total_samples})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063becfe",
   "metadata": {},
   "source": [
    "## with oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bde0e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── prepare dataset ──────────────────────────────────────────────────────\n",
    "train_set = AudioDataset(\"./data/train.csv\", \"./data_raw/train/audio\")\n",
    "labels = [lbl for _, lbl in train_set]              # list of ints\n",
    "counts = torch.bincount(torch.tensor(labels))       # #samples per class\n",
    "\n",
    "# ─── “soft” oversampling: cap max weight at e.g. 3× minority vs majority ──\n",
    "inv = 1.0 / counts.float()                          # raw inverse freq\n",
    "max_mult = 3.0\n",
    "capped = torch.minimum(inv, inv.max() * max_mult)\n",
    "\n",
    "sample_weights = [capped[lbl].item() for lbl in labels]\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=256,\n",
    "    sampler=sampler,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# ─── class‑weighted loss to keep natural skew partially ─────────────────\n",
    "class_weights = (1.0 / counts.float()).to(device)\n",
    "class_weights = torch.minimum(class_weights, class_weights.max() * max_mult)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# ─── test loader ────────────────────────────────────────────────────────\n",
    "test_set = AudioDataset(\"./data/test.csv\", \"./data_raw/train/audio\")\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8437124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class  0:  8.49% (4347/51200)\n",
      "class  1:  8.19% (4192/51200)\n",
      "class  2:  8.41% (4308/51200)\n",
      "class  3:  8.31% (4257/51200)\n",
      "class  4:  8.41% (4307/51200)\n",
      "class  5:  8.42% (4310/51200)\n",
      "class  6:  8.39% (4295/51200)\n",
      "class  7:  8.21% (4206/51200)\n",
      "class  8:  8.55% (4377/51200)\n",
      "class  9:  8.27% (4236/51200)\n",
      "class 10:  8.23% (4216/51200)\n",
      "class 11:  8.10% (4149/51200)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "# ─── count labels in one epoch ─────────────────────────────────────────\n",
    "label_counts = Counter()\n",
    "total_samples = 0\n",
    "\n",
    "for specs, labels in train_loader:\n",
    "    labels = labels.cpu().tolist()\n",
    "    label_counts.update(labels)\n",
    "    total_samples += len(labels)\n",
    "\n",
    "# ─── print percentage per class ─────────────────────────────────────────\n",
    "for cls_idx, count in sorted(label_counts.items()):\n",
    "    pct = count / total_samples * 100\n",
    "    print(f\"class {cls_idx:2d}: {pct:5.2f}% ({count}/{total_samples})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb63a504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
